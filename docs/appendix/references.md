# 參考資源

LLM 評測相關的工具、論文、課程等參考資源彙整。

## 評測工具

### 開源框架

| 工具 | 說明 | 連結 |
|------|------|------|
| RAGAS | RAG 評測框架 | [GitHub](https://github.com/explodinggradients/ragas) |
| DeepEval | LLM 測試框架 | [GitHub](https://github.com/confident-ai/deepeval) |
| Arize Phoenix | ML 可觀測性 | [GitHub](https://github.com/Arize-ai/phoenix) |
| LangSmith | LangChain 監控 | [官網](https://smith.langchain.com/) |
| Promptfoo | Prompt 測試 | [GitHub](https://github.com/promptfoo/promptfoo) |

### 商業平台

| 平台 | 特色 |
|------|------|
| Weights & Biases | 實驗追蹤 |
| Humanloop | Prompt 管理 |
| Braintrust | AI 評測 |

---

## 重要論文

### 評測方法論

- [Judging LLM-as-a-Judge](https://arxiv.org/abs/2306.05685) - LLM 作為評估者
- [G-Eval](https://arxiv.org/abs/2303.16634) - GPT-4 評估框架
- [RAGAS Paper](https://arxiv.org/abs/2309.15217) - RAG 評測指標

### 安全性

- [Red Teaming LLMs](https://arxiv.org/abs/2202.03286) - LLM 紅隊測試
- [Jailbreaking ChatGPT](https://arxiv.org/abs/2305.13860) - 越獄攻擊研究

### 基準測試集

- [MMLU](https://arxiv.org/abs/2009.03300) - 多任務語言理解
- [TruthfulQA](https://arxiv.org/abs/2109.07958) - 真實性評測
- [HumanEval](https://arxiv.org/abs/2107.03374) - 程式碼生成

---

## 學習資源

### 課程

- [DeepLearning.AI - Evaluating LLMs](https://www.deeplearning.ai/)
- [LangChain Evaluation Guide](https://python.langchain.com/docs/guides/evaluation)

### 部落格

- [Anthropic Research Blog](https://www.anthropic.com/research)
- [OpenAI Blog](https://openai.com/blog)
- [Hugging Face Blog](https://huggingface.co/blog)

---

## 社群

- [AI Safety Community](https://www.aisafety.camp/)
- [LLM Security Discord](https://discord.gg/llmsecurity)
- [MLOps Community](https://mlops.community/)
